{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4da88593",
   "metadata": {},
   "source": [
    "# 第九週實驗課：語音轉文字、大型語言模型與文字轉語音（含聲音複製）\n",
    "\n",
    "本次實驗課目標：\n",
    "1.  **語音轉文字 (S2T)**：將聲音轉成文字。\n",
    "2.  **大型語言模型 (LLM)**：用來修正文字文法。\n",
    "3.  **文字轉語音 (TTS) 與聲音複製**：將文字轉成特定人聲的語音。\n",
    "\n",
    "我們會先分別測試，再整合成一個專案。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f67538",
   "metadata": {},
   "source": [
    "## 安裝所需套件\n",
    "\n",
    "執行此區塊來安裝本次實驗所需的 Python 套件。\n",
    "`%capture` 會隱藏安裝過程的輸出訊息。\n",
    "\n",
    "%%capture install_logs\n",
    "!pip install f5-tts sounddevice wavio ipywebrtc notebook autoawq -q # 安裝主要的 Python 套件 (-q 安靜模式)\n",
    "# !pip install git+https://github.com/openai/whisper.git # 可選：安裝最新版 Whisper\n",
    "# !pip install flash-attn --no-build-isolation # 可選：若環境支援可加速模型\n",
    "!apt install ffmpeg libportaudio2 -y -qq # 安裝系統工具 ffmpeg 和 PortAudio (-qq 更安靜)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58133527",
   "metadata": {},
   "source": [
    "## 測試文字轉語音 (TTS) Gradio 介面 (可選)\n",
    "\n",
    "`f5-tts` 提供了一個網頁介面方便快速測試。取消下一行的註解並執行，會產生一個公開網址。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaf101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !f5-tts_infer-gradio --share # 啟動 Gradio 介面"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df189d4",
   "metadata": {},
   "source": [
    "## 匯入所需模組\n",
    "\n",
    "匯入程式中會用到的 Python 模組。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbffa923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import shlex\n",
    "import subprocess\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') # 忽略警告訊息\n",
    "\n",
    "from rich import print as rprint # 美化 print 輸出\n",
    "import gdown # 從 Google Drive 下載\n",
    "from huggingface_hub import notebook_login # Hugging Face 登入\n",
    "import IPython # 顯示音訊播放器等\n",
    "import torch # PyTorch 深度學習框架\n",
    "from transformers import ( # Hugging Face 函式庫\n",
    "    AutoProcessor, AutoTokenizer, TextGenerationPipeline,\n",
    "    AutoModelForSpeechSeq2Seq, AutomaticSpeechRecognitionPipeline,\n",
    "    pipeline, BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "# 檢查是否有 GPU 可用\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"使用的運算裝置 (Device): {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a522e3",
   "metadata": {},
   "source": [
    "## Hugging Face Hub 登入 (非必要)\n",
    "\n",
    "若要使用需要授權的模型 (如 Llama 3)，請執行 `notebook_login()`。若只用公開模型則跳過。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c08db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdaba58",
   "metadata": {},
   "source": [
    "## 下載範例聲音檔案\n",
    "\n",
    "從 Google Drive 下載包含多個角色聲音的範例檔案夾。\n",
    "`!ls` 用來確認檔案已下載。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916584df",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://drive.google.com/drive/folders/1dq-koI-P-tYJDfE1DtLdNZrUdCJLwcQZ?usp=sharing\"\n",
    "gdown.download_folder(url, quiet=True, use_cookies=False)\n",
    "!ls week9/ # 列出下載的資料夾內容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46f08d4",
   "metadata": {},
   "source": [
    "## 錄製你自己的聲音 (可選)\n",
    "\n",
    "使用互動小工具錄音。點擊圓鈕開始/停止。\n",
    "**停止錄音後，請執行下一個儲存區塊。**\n",
    "(注意：此小工具在某些環境可能無法運作)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d9e245",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n",
    "except ImportError:\n",
    "    print(\"偵測不到 google.colab.output，互動小工具可能自動啟用或需要不同設定。\")\n",
    "\n",
    "from ipywebrtc import CameraStream, AudioRecorder\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Audio\n",
    "\n",
    "camera = CameraStream(constraints={'audio': True, 'video': False}) # 只取聲音\n",
    "recorder = AudioRecorder(stream=camera)\n",
    "\n",
    "print(\"請點擊下方的圓形按鈕開始錄音，再次點擊則停止錄音。\")\n",
    "display(recorder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f828bef",
   "metadata": {},
   "source": [
    "## 儲存錄音檔\n",
    "\n",
    "**請在上方停止錄音後，才執行此區塊。**\n",
    "將錄製的聲音 (通常是 webm 格式) 存檔。\n",
    "下方註解提供轉換成 WAV 格式的 `ffmpeg` 指令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc9110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_kaggle_widget_recording(filename: str = \"my_recording.webm\"): # 改用更有意義的預設檔名\n",
    "  \"\"\"將 ipywebrtc 小工具錄製的音訊儲存到檔案。\"\"\"\n",
    "  if not recorder.audio.value:\n",
    "    print(\"錯誤：沒有錄到聲音。\")\n",
    "    return\n",
    "  try:\n",
    "    audio_bytes = recorder.audio.value\n",
    "    with open(filename, 'wb') as f:\n",
    "      f.write(audio_bytes)\n",
    "    print(f\"錄音已儲存為 {filename}\")\n",
    "    display(Audio(filename)) # 顯示播放器\n",
    "  except Exception as e:\n",
    "    print(f\"儲存時發生錯誤: {e}\")\n",
    "\n",
    "# 儲存錄音\n",
    "save_kaggle_widget_recording() # 使用預設檔名 \"my_recording.webm\"\n",
    "\n",
    "# --- 選擇性：將錄音檔轉為 WAV 格式 ---\n",
    "# !ffmpeg -i my_recording.webm -ac 1 -f wav my_recording.wav -y -hide_banner -loglevel error\n",
    "# if $? -eq 0 ; then echo \"成功轉換為 my_recording.wav\"; display(Audio(\"my_recording.wav\")); fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56559d0c",
   "metadata": {},
   "source": [
    "## 語音轉文字 (Speech-to-Text, S2T)\n",
    "\n",
    "使用 OpenAI Whisper 模型將聲音轉成文字。\n",
    "`prepare_whisper_model` 函式會載入模型並設定好 pipeline。\n",
    "`@functools.cache` 用於快取模型，避免重複載入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f29de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.cache\n",
    "def prepare_whisper_model(model_id: str = \"openai/whisper-large-v3\") -> AutomaticSpeechRecognitionPipeline: # 改用 v3，turbo 可能非公開\n",
    "    \"\"\"載入並設定 Whisper S2T 模型 pipeline。\"\"\"\n",
    "    print(f\"正在載入 Whisper 模型: {model_id} ...\")\n",
    "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch_dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        attn_implementation=\"sdpa\" # 使用優化的注意力機制\n",
    "    )\n",
    "    model.to(device)\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        chunk_length_s=30, # 處理 30 秒聲音片段\n",
    "        batch_size=16,     # 可依 GPU 記憶體調整\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    "    )\n",
    "    print(\"Whisper 模型載入完成。\")\n",
    "    return pipe\n",
    "\n",
    "# 載入 S2T 模型\n",
    "s2t = prepare_whisper_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d984b984",
   "metadata": {},
   "source": [
    "### 執行語音轉文字\n",
    "\n",
    "將 `week9` 資料夾中的第一個 `.wav` 檔進行辨識。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eeb0c8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 取得 week9 資料夾中所有 wav 檔案路徑\n",
    "audio_files = list(Path(\"./week9\").glob(\"*.wav\"))\n",
    "if not audio_files:\n",
    "    print(\"錯誤：在 week9 資料夾中找不到任何 .wav 檔案。\")\n",
    "else:\n",
    "    first_audio_path = str(audio_files[0].resolve())\n",
    "    print(\"找到的聲音檔案路徑 (部分):\")\n",
    "    rprint([str(p) for p in audio_files[:5]]) # 只印出前 5 個\n",
    "\n",
    "    print(f\"\\n播放第一個聲音檔: {first_audio_path}\")\n",
    "    display(IPython.display.Audio(first_audio_path))\n",
    "\n",
    "    print(\"\\n正在進行語音轉文字...\")\n",
    "    transcription = s2t(first_audio_path) # 對第一個檔案進行辨識\n",
    "    print(\"\\n辨識結果:\")\n",
    "    rprint(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd265445",
   "metadata": {},
   "source": [
    "## 大型語言模型 (LLM) - 文法修正\n",
    "\n",
    "使用 LLM (例如 Qwen) 來修正 S2T 結果的文法錯誤。\n",
    "`prepare_llm` 函式載入 LLM pipeline。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9846c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.cache\n",
    "def prepare_llm(model_id: str = \"Qwen/Qwen1.5-7B-Chat-AWQ\") -> TextGenerationPipeline: # 更新為 Qwen1.5 AWQ 模型\n",
    "    \"\"\"載入並設定 LLM 文字生成 pipeline。\"\"\"\n",
    "    print(f\"正在載入 LLM: {model_id} ...\")\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_id,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\", # 自動分配裝置\n",
    "    )\n",
    "    print(\"LLM 載入完成。\")\n",
    "    return pipe\n",
    "\n",
    "# 載入 LLM\n",
    "llm = prepare_llm()\n",
    "tokenizer = llm.tokenizer # 取得分詞器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b49f83",
   "metadata": {},
   "source": [
    "### 測試 LLM 文法修正\n",
    "\n",
    "準備中英文錯誤句子範例，並定義一個函式 `prepare_grammar_correction_messages` 來建立 LLM 的輸入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d85751",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 英文錯誤範例\n",
    "english_sentences = [\n",
    "    {\"incorrect\": \"Their going too the park later.\", \"correct\": \"They're going to the park later.\"},\n",
    "    {\"incorrect\": \"Me and him discussed it.\", \"correct\": \"He and I discussed it.\"},\n",
    "]\n",
    "# 中文錯誤範例 (臺灣用語)\n",
    "chinese_sentences = [\n",
    "    {\"incorrect\": \"我明天在去。\", \"correct\": \"我明天再去。\"},\n",
    "    {\"incorrect\": \"這見衣服必較好看。\", \"correct\": \"這件衣服比較好看。\"},\n",
    "]\n",
    "\n",
    "def prepare_grammar_correction_messages(text: str) -> list[dict]:\n",
    "    \"\"\"建立 LLM 文法修正任務的 messages 列表。\"\"\"\n",
    "    system_message = {\"role\": \"system\", \"content\": \"你是一位專業的編輯，擅長修正英文和繁體中文的文法與拼寫錯誤。請直接提供修正後的文字，不要包含任何解釋。\"}\n",
    "    user_message = {\"role\": \"user\", \"content\": f\"請修正以下文字的文法與拼寫錯誤：\\n```\\n{text}\\n```\"}\n",
    "    return [system_message, user_message]\n",
    "\n",
    "# --- 測試英文 ---\n",
    "print(\"--- 測試英文修正 ---\")\n",
    "text_en_incorrect = english_sentences[0][\"incorrect\"]\n",
    "print(f\"原始: {text_en_incorrect}\")\n",
    "messages_en = prepare_grammar_correction_messages(text_en_incorrect)\n",
    "res_en = llm(messages_en, max_new_tokens=50, return_full_text=False)\n",
    "print(f\"修正: {res_en[0]['generated_text'].strip()}\")\n",
    "print(f\"預期: {english_sentences[0]['correct']}\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# --- 測試中文 ---\n",
    "print(\"--- 測試中文修正 ---\")\n",
    "text_zh_incorrect = chinese_sentences[1][\"incorrect\"]\n",
    "print(f\"原始: {text_zh_incorrect}\")\n",
    "messages_zh = prepare_grammar_correction_messages(text_zh_incorrect)\n",
    "res_zh = llm(messages_zh, max_new_tokens=50, return_full_text=False)\n",
    "print(f\"修正: {res_zh[0]['generated_text'].strip()}\")\n",
    "print(f\"預期: {chinese_sentences[1]['correct']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df16c02",
   "metadata": {},
   "source": [
    "## 文字轉語音 (TTS) 與聲音複製\n",
    "\n",
    "使用 `f5-tts` 命令列工具，將文字轉換成指定參考聲音 (`ref_audio`) 的語音。\n",
    "`clone_voice` 函式封裝了執行此命令的過程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f358d14",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def clone_voice(path_to_ref_audio: str,\n",
    "                gen_text: str,\n",
    "                ref_text: str = \"\", # 參考音檔的文字稿 (選填，但建議提供)\n",
    "                output_file: str = \"tts_output.wav\",\n",
    "                output_dir: str = \"tts_output\"): # 改用更有意義的預設目錄\n",
    "    \"\"\"使用 f5-tts 命令列工具進行聲音複製。\"\"\"\n",
    "    cli_executable = \"f5-tts_infer-cli\"\n",
    "    model = \"F5TTS_v1_Base\"\n",
    "    output_path = Path(output_dir) / output_file\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True) # 建立輸出目錄\n",
    "\n",
    "    command = [\n",
    "        cli_executable, \"--model\", model,\n",
    "        \"--ref_audio\", str(Path(path_to_ref_audio).resolve()),\n",
    "        \"--ref_text\", ref_text,\n",
    "        \"--gen_text\", gen_text,\n",
    "        \"--output_dir\", str(output_path.parent.resolve()),\n",
    "        \"--output_file\", output_path.name,\n",
    "        \"--nfe_step\", \"64\", # 推論步數\n",
    "        \"--device\", device,\n",
    "    ]\n",
    "    print(f\"準備執行 TTS 命令: {shlex.join(command)}\")\n",
    "    try:\n",
    "        result = subprocess.run(command, check=True, capture_output=True, text=True, encoding='utf-8')\n",
    "        print(\"\\nTTS 命令執行成功。\")\n",
    "        print(f\"生成的音訊檔案: {output_path}\")\n",
    "        display(IPython.display.Audio(str(output_path))) # 顯示播放器\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"\\nTTS 命令執行失敗 (錯誤碼 {e.returncode}):\\nstderr: {e.stderr}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\n錯誤：找不到命令 '{cli_executable}'。請確認 f5-tts 已安裝。\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n執行 TTS 時發生錯誤: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e39026c",
   "metadata": {},
   "source": [
    "### 測試聲音複製\n",
    "\n",
    "載入範例聲音的文字稿 (`transcripts.json`)，然後選擇一個聲音 (如 Walken) 來念一段指定的中文文字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c43fb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 載入文字稿\n",
    "transcript_file = Path(\"./week9/transcripts.json\")\n",
    "transcripts = {} # 初始化為空字典\n",
    "if transcript_file.exists():\n",
    "    try:\n",
    "        with open(transcript_file, 'r', encoding='utf-8') as f:\n",
    "            transcripts = json.load(f)\n",
    "        print(\"已載入文字稿檔案。\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"錯誤：無法解析文字稿檔案 {transcript_file}。\")\n",
    "else:\n",
    "    print(f\"警告：找不到文字稿檔案 {transcript_file}。TTS 的 ref_text 將會是空的。\")\n",
    "\n",
    "# --- 選擇聲音並執行 TTS ---\n",
    "voice_key = \"walken.wav\" # 選擇要模仿的聲音檔名\n",
    "ref_audio_path_str = f\"./week9/{voice_key}\" # 使用相對路徑更通用\n",
    "ref_audio_path = Path(ref_audio_path_str)\n",
    "\n",
    "if ref_audio_path.exists():\n",
    "    ref_transcription = transcripts.get(voice_key, {}).get(\"transcription\", \"\") # 安全地取得文字稿\n",
    "    print(f\"\\n選擇模仿的聲音: {voice_key}\")\n",
    "    if not ref_transcription:\n",
    "        print(\"(警告：找不到此聲音的文字稿，ref_text 將為空)\")\n",
    "\n",
    "    text_to_generate = \"白日依山盡，黃河入海流。欲窮千里目，更上一層樓。\"\n",
    "    print(f\"要生成的文字: {text_to_generate}\")\n",
    "\n",
    "    clone_voice(\n",
    "        path_to_ref_audio=str(ref_audio_path), # 傳入字串路徑\n",
    "        ref_text=ref_transcription,\n",
    "        gen_text=text_to_generate,\n",
    "        output_file=f\"{voice_key.split('.')[0]}_poem.wav\" # 更有意義的檔名\n",
    "    )\n",
    "else:\n",
    "    print(f\"錯誤：找不到參考聲音檔案 {ref_audio_path_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a17e5a6",
   "metadata": {},
   "source": [
    "## 整合：從輸入到輸出\n",
    "\n",
    "`read_for_me` 函式整合了 S2T、LLM 修正、TTS 聲音複製的完整流程。\n",
    "輸入可以是文字 (`input_text`) 或聲音檔 (`input_audio_path`)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d2090",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def read_for_me(\n",
    "    s2t_model: AutomaticSpeechRecognitionPipeline,\n",
    "    llm_model: TextGenerationPipeline,\n",
    "    transcripts_dict: dict, # 改為必要參數\n",
    "    input_text: str | None = None,\n",
    "    input_audio_path: str | None = None,\n",
    "    voice_to_clone_key: str = \"spongebob.wav\", # 預設模仿海綿寶寶\n",
    "    output_file: str = \"final_output.wav\",\n",
    "    output_dir: str = \"tts_final\"\n",
    "):\n",
    "    \"\"\"整合 S2T -> LLM -> TTS 的流程。\"\"\"\n",
    "    # --- 1. 檢查與取得輸入文字 ---\n",
    "    if not input_text and not input_audio_path:\n",
    "        raise ValueError(\"必須提供 input_text 或 input_audio_path。\")\n",
    "    if input_text and input_audio_path:\n",
    "        raise ValueError(\"input_text 和 input_audio_path 不能同時提供。\")\n",
    "    if voice_to_clone_key not in transcripts_dict:\n",
    "        print(f\"警告：在文字稿字典中找不到 key '{voice_to_clone_key}'。\")\n",
    "        # raise ValueError(f\"在 transcripts_dict 中找不到 key: '{voice_to_clone_key}'\")\n",
    "\n",
    "    original_text = \"\"\n",
    "    if input_audio_path:\n",
    "        print(f\"步驟 1: S2T ({input_audio_path})\")\n",
    "        audio_file = Path(input_audio_path)\n",
    "        if not audio_file.exists(): raise FileNotFoundError(f\"找不到音檔: {input_audio_path}\")\n",
    "        original_text = s2t_model(str(audio_file))[\"text\"]\n",
    "        print(f\"  >> S2T 結果: {original_text}\")\n",
    "    else:\n",
    "        original_text = input_text\n",
    "        print(f\"步驟 1: 使用輸入文字: {original_text}\")\n",
    "\n",
    "    # --- 2. LLM 文法修正 ---\n",
    "    print(\"\\n步驟 2: LLM 文法修正\")\n",
    "    messages = prepare_grammar_correction_messages(original_text)\n",
    "    # 增加溫度參數讓輸出稍微多樣化，並設定停止符號 (如果模型支援)\n",
    "    llm_result = llm_model(messages, return_full_text=False, max_new_tokens=len(original_text) + 50,\n",
    "                           temperature=0.7, eos_token_id=tokenizer.eos_token_id)\n",
    "    corrected_text = llm_result[0][\"generated_text\"].strip()\n",
    "    # 清理常見的模型輸出問題 (例如多餘的引號或標籤)\n",
    "    corrected_text = corrected_text.replace(\"```\", \"\").replace(\"`\", \"\").strip('\"').strip()\n",
    "    print(f\"  >> 修正後文字: {corrected_text}\")\n",
    "\n",
    "    # --- 3. TTS 聲音複製 ---\n",
    "    print(f\"\\n步驟 3: TTS 聲音複製 (模仿 {voice_to_clone_key})\")\n",
    "    voice_info = transcripts_dict.get(voice_to_clone_key, {}) # 安全取得\n",
    "    voice_ref_path_str = f\"./week9/{voice_to_clone_key}\"\n",
    "    voice_ref_path = Path(voice_ref_path_str)\n",
    "    voice_ref_text = voice_info.get(\"transcription\", \"\")\n",
    "\n",
    "    if not voice_ref_path.exists():\n",
    "        raise FileNotFoundError(f\"找不到參考聲音檔: {voice_ref_path_str}\")\n",
    "    if not voice_ref_text:\n",
    "         print(\"(警告：找不到參考聲音文字稿，ref_text 將為空)\")\n",
    "\n",
    "    clone_voice(\n",
    "        path_to_ref_audio=str(voice_ref_path),\n",
    "        gen_text=corrected_text,\n",
    "        ref_text=voice_ref_text,\n",
    "        output_file=output_file,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    print(f\"\\n流程完成！最終語音已儲存至 {Path(output_dir) / output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1c8d2a",
   "metadata": {},
   "source": [
    "### 執行整合流程\n",
    "\n",
    "測試 `read_for_me` 函式。\n",
    "範例 1 使用文字輸入，範例 2 嘗試使用之前錄製的聲音檔。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d78a999",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 執行整合流程 - 範例 1: 文字輸入，海綿寶寶聲音\n",
    "print(\"--- 執行整合流程：範例 1 (文字輸入) ---\")\n",
    "try:\n",
    "    read_for_me(\n",
    "        s2t_model=s2t,\n",
    "        llm_model=llm,\n",
    "        transcripts_dict=transcripts, # 傳入文字稿字典\n",
    "        input_text=\"i has a apple.\", # 簡單錯誤英文\n",
    "        voice_to_clone_key=\"spongebob.wav\",\n",
    "        output_file=\"spongebob_corrected_apple.wav\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"執行範例 1 時發生錯誤: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# 執行整合流程 - 範例 2: 聲音輸入 (若存在)，川普聲音\n",
    "print(\"--- 執行整合流程：範例 2 (聲音輸入) ---\")\n",
    "my_recording_wav_path = \"my_recording.wav\" # 假設已轉換成 WAV\n",
    "my_recording_webm_path = \"my_recording.webm\" # 或者使用 webm\n",
    "\n",
    "input_audio_for_test = None\n",
    "if Path(my_recording_wav_path).exists():\n",
    "    input_audio_for_test = my_recording_wav_path\n",
    "elif Path(my_recording_webm_path).exists():\n",
    "     input_audio_for_test = my_recording_webm_path # Whisper 通常能處理 webm\n",
    "\n",
    "if input_audio_for_test:\n",
    "    print(f\"使用錄音檔: {input_audio_for_test}\")\n",
    "    try:\n",
    "        read_for_me(\n",
    "            s2t_model=s2t,\n",
    "            llm_model=llm,\n",
    "            transcripts_dict=transcripts,\n",
    "            input_audio_path=input_audio_for_test, # 使用錄音檔\n",
    "            voice_to_clone_key=\"trump.wav\",     # 模仿川普\n",
    "            output_file=\"my_recording_trump_voice.wav\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"執行範例 2 時發生錯誤: {e}\")\n",
    "else:\n",
    "    print(f\"找不到錄音檔 ({my_recording_wav_path} 或 {my_recording_webm_path})，跳過範例 2。\")\n",
    "    print(\"請確認已成功錄音並執行儲存步驟。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f773b1c",
   "metadata": {},
   "source": [
    "## 使用 Gradio 錄製你自己的聲音\n",
    "\n",
    "這個區塊會啟動一個簡單的 Gradio 應用程式，讓你可以：\n",
    "1.  點擊「錄音」按鈕開始錄音。\n",
    "2.  再次點擊停止錄音。\n",
    "3.  錄音會自動儲存成 `.wav` 檔案。\n",
    "4.  介面會顯示儲存的檔案路徑，並提供播放和下載的選項。\n",
    "\n",
    "**使用方式：**\n",
    "* 執行這個 Python 程式碼區塊。\n",
    "* 在出現的 Gradio 介面中錄製聲音。\n",
    "* 錄音完成後，複製顯示的 `.wav` 檔案路徑\n",
    "* 你可以在後面的 `read_for_me` 函式呼叫中，將這個路徑作為 `input_audio_path` 參數的值來使用。\n",
    "* **注意：** 錄音完成後，這個 Gradio App 會持續運行。你可以在完成錄音後手動停止這個 Cell 的執行，或者讓它繼續運行直到你關閉 Notebook。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7f2b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_recording(audio):\n",
    "    \"\"\"\n",
    "    處理 Gradio 音訊輸入，儲存為 WAV 檔案。\n",
    "\n",
    "    Args:\n",
    "        audio: Gradio Audio 元件的回傳值 (設定 type=\"numpy\" 時為 (sample_rate, data) tuple)。\n",
    "\n",
    "    Returns:\n",
    "        儲存的 WAV 檔案路徑。\n",
    "    \"\"\"\n",
    "    if audio is None:\n",
    "        return \"錯誤：未偵測到音訊輸入。\"\n",
    "\n",
    "    sample_rate, data = audio\n",
    "\n",
    "    # 確保 data 是 NumPy array 且為適當的 dtype (例如 int16)\n",
    "    if not isinstance(data, np.ndarray):\n",
    "         return f\"錯誤：音訊資料類型不正確 ({type(data)})，應為 NumPy array。\"\n",
    "\n",
    "    # 將音訊資料轉換為 int16 (常見的 WAV 格式)\n",
    "    # 先檢查最大值以避免 clipping\n",
    "    max_val = np.max(np.abs(data))\n",
    "    if max_val > 0:\n",
    "        data_int16 = (data / max_val * 32767).astype(np.int16)\n",
    "    else:\n",
    "        data_int16 = data.astype(np.int16) # 如果是靜音\n",
    "\n",
    "    # 產生帶有時間戳的檔名\n",
    "    filename = f\"my_recording.wav\"\n",
    "    filepath = Path(\"./\") / filename # 儲存在目前工作目錄\n",
    "\n",
    "    try:\n",
    "        # 使用 wavio 儲存 WAV 檔案\n",
    "        wavio.write(str(filepath), data_int16, sample_rate, sampwidth=2) # sampwidth=2 for 16-bit\n",
    "        print(f\"錄音已儲存至：{filepath}\")\n",
    "        return str(filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"儲存 WAV 檔案時發生錯誤：{e}\")\n",
    "        return f\"儲存錯誤：{e}\"\n",
    "\n",
    "# 建立 Gradio 介面\n",
    "# inputs: 麥克風錄音，回傳 numpy array (sample_rate, data)\n",
    "# outputs: 顯示檔案路徑 (gr.File) 和播放器 (gr.Audio)\n",
    "recorder_app = gr.Interface(\n",
    "    fn=save_recording,\n",
    "    inputs=gr.Audio(sources=[\"microphone\"], type=\"numpy\", label=\"點此錄音 (Click to Record)\"),\n",
    "    outputs=[\n",
    "        gr.File(label=\"儲存的 WAV 檔案 (Saved WAV File)\"),\n",
    "        gr.Audio(label=\"播放錄音 (Playback Recording)\")\n",
    "    ],\n",
    "    title=\"簡易錄音機 (Simple Audio Recorder)\",\n",
    "    description=\"錄製聲音並儲存為 WAV 檔案。錄音後，檔案路徑會顯示在下方，可供下載或複製路徑用於後續步驟。\",\n",
    "    allow_flagging=\"never\"\n",
    ")\n",
    "\n",
    "# 啟動 Gradio App\n",
    "# share=True 會產生公開連結，方便在 Colab/Kaggle 等環境使用\n",
    "# inline=True 會嘗試在 Notebook 中內嵌顯示 (不一定所有環境都支援)\n",
    "recorder_app.launch(share=True, inline=False) # 建議 share=True, inline=False"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
